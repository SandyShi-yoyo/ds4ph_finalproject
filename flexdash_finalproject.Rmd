---
title: "Diabetes Prediction"
output: 
  flexdashboard::flex_dashboard:
    orientation: columns
    vertical_layout: fill
runtime: shiny
---

```{r setup, include=FALSE}
library(flexdashboard)
library(dplyr)
library(readr)
library(klaR) # Naïve Bayes
library(e1071) # SVM
library(ROCR) # ROC curves and AUC
library(VennDiagram)
library(RColorBrewer)
library(vioplot)
library(hfunk)
library(gplots) # heatmap.2
library(caret) # confusionmatrix
library(randomForest)
library(plotly)
library(corrplot)


```

```{r warning=FALSE, message=FALSE}
data1 = read_csv('diabetes-dataset.csv')
data2 = read_csv('diabetes_data_upload.csv')

draw_confusion_matrix <- function(cm) {
  layout(matrix(c(1,1,2)))
  par(mar=c(2,2,2,2))
  plot(c(100, 345), c(300, 450), type = "n", xlab="", ylab="", xaxt='n', yaxt='n')
  title('CONFUSION MATRIX', cex.main=2)
  # create the matrix 
  rect(150, 430, 240, 370, col='#3F97D0')
  text(195, 435, 'Class1', cex=1.2)
  rect(250, 430, 340, 370, col='#F7AD50')
  text(295, 435, 'Class2', cex=1.2)
  text(125, 370, 'Predicted', cex=1.3, srt=90, font=2)
  text(245, 450, 'Actual', cex=1.3, font=2)
  rect(150, 305, 240, 365, col='#F7AD50')
  rect(250, 305, 340, 365, col='#3F97D0')
  text(140, 400, 'Class1', cex=1.2, srt=90)
  text(140, 335, 'Class2', cex=1.2, srt=90)

  # add in the cm results 
  res <- as.numeric(cm$table)
  text(195, 400, res[1], cex=1.6, font=2, col='white')
  text(195, 335, res[2], cex=1.6, font=2, col='white')
  text(295, 400, res[3], cex=1.6, font=2, col='white')
  text(295, 335, res[4], cex=1.6, font=2, col='white')

  # add in the specifics 
  plot(c(100, 0), c(100, 0), type = "n", xlab="", ylab="", main = "DETAILS", xaxt='n', yaxt='n')
  text(10, 85, names(cm$byClass[1]), cex=1.2, font=2)
  text(10, 70, round(as.numeric(cm$byClass[1]), 3), cex=1.2)
  text(30, 85, names(cm$byClass[2]), cex=1.2, font=2)
  text(30, 70, round(as.numeric(cm$byClass[2]), 3), cex=1.2)
  text(50, 85, names(cm$byClass[5]), cex=1.2, font=2)
  text(50, 70, round(as.numeric(cm$byClass[5]), 3), cex=1.2)
  text(70, 85, names(cm$byClass[6]), cex=1.2, font=2)
  text(70, 70, round(as.numeric(cm$byClass[6]), 3), cex=1.2)
  text(90, 85, names(cm$byClass[7]), cex=1.2, font=2)
  text(90, 70, round(as.numeric(cm$byClass[7]), 3), cex=1.2)

  # add in the accuracy information 
  text(30, 35, names(cm$overall[1]), cex=1.5, font=2)
  text(30, 20, round(as.numeric(cm$overall[1]), 3), cex=1.4)
  text(70, 35, names(cm$overall[2]), cex=1.5, font=2)
  text(70, 20, round(as.numeric(cm$overall[2]), 3), cex=1.4)
}  
```

Overview
=======================================================================

Row {data-height=700}
-----------------------------------------------------------------------

### Plot 1. Age-adjusted prevalence of diagnosed diabetes by detailed race and sex among adults aged 18 years or older, United States, 2017–2018.
```{r}
renderPlotly({
  dat1 = read_csv('https://raw.githubusercontent.com/zou2/Test/main/Overview_gender.csv')
  # dat1 = read_csv('Overview_gender.csv')
  g1 = ggplot(dat1,aes(x = as.factor(Gender), 
                   y = Prevalence,
                   fill = as.factor(Race)))
  g1 = g1 + geom_col()
  G1 = ggplotly(g1)
  G1
})
```

### Plot 2. Trends in age-adjusted prevalence of diagnosed, undiagnosed, and total diabetes among adults aged 18 years or older, United States, 1999–2016. 
```{r}
renderPlotly({
  dat2 = read_csv('https://raw.githubusercontent.com/zou2/Test/main/Overview_diag.csv')
  g2 = ggplot(dat2,aes(x = Time,
                   y = Prevalence,
                   group = as.factor(Type),
                   color = as.factor(Type)))
  g2 = g2 + geom_line()+theme(axis.text.x=element_text(angle=90,hjust=1))
  G2 = ggplotly(g2)
  G2
})
```

Row {data-height=300}
-----------------------------------------------------------------------

### About the overview
- Plot 1 shows the prevalence of diabetes in people with different gender and race in the US between 2017 and 2018.
- Plot 2 shows the prevalence of both the diagnosed and undiagnosed diabetes among people who are 18 years or older in the US between 1999 and 2016.
- Both of the plots are generated from "the National Diabetes Statistics Report 2020. Estimates of diabetes and its burden in the United States" pubslished by CDC.


Model Evalutation on 2000 Patient Data
=======================================================================

Row {data-height=200}
-----------------------------------------------------------------------

### Plot 0. Predictor Correlation Observation
```{r}
data1<-data1[complete.cases(data1),]
data1$Outcome<-as.numeric(data1$Outcome)
M <-cor(data1)
corrplot(M, type="upper", order="hclust",
         col=brewer.pal(n=8, name="RdYlBu"))

```

### About Model Evaluation

- According to Plot 1, the SVM model reports an accuracy of 0.828 on 400 randomly selected test patient data. With 5-Folds Cross Validation, the ROCs are shown in Plot 2, and the average AUC in all folds is above 0.8.  
- According to Plot 3, the Random Forest model reports an accuracy of 1.000 on 400 randomly selected test patient data. With 5-Folds Cross Validation, the ROCs are shown in Plot 4, and the average AUC in all folds is 1. 

Row {data-height=300}
-----------------------------------------------------------------------

### Plot 1. SVM Prediction Model Confusion Matrix
```{r}
# SVM
data1$Outcome = as.factor(data1$Outcome)
testidx <- which(1:(dim(data1[,1])[1])%%5 == 0)
dftrain <- data1[-testidx,]
dftest <- data1[testidx,]
# hist(as.numeric(dftrain$Outcome))
# hist(as.numeric(dftest$Outcome))
svm_model <- svm(Outcome~., data = dftrain, probability = T, kernel="polynomial", degree=5)
svm_prediction <- predict(svm_model, dftest, probability = T)
# table(predicted=svm_prediction, observed=dftest$Outcome)
cmsvm=confusionMatrix(svm_prediction, dftest$Outcome, positive = "1")
draw_confusion_matrix(cmsvm)
```

### Plot 2. SVM Prediction Model 5 Folds Cross Validation ROC
```{r}
# Repeat using 5fold cross validation. Split data into 5 equally sized parts
folds <- function(x,n) split(x, sort(rep(1:n,len=length(x))))
crossdata <- folds(data1,5)
pred_label <- list()
actual_label <- list()
auc_list <- list()

for (i in 1:5) {
  test <- do.call("rbind", crossdata[i])
  train <- do.call("rbind", crossdata[-i])
  # print((train))
  model <- svm(Outcome~., data = train, probability = T, kernel="linear")

  prediction <- predict(model, test, probability = T)
  pred_class_svm <- attr(prediction, "probabilities")[, "1"]
  table(test$Outcome, pred_class_svm)
  pred_label[[i]] <- pred_class_svm
  actual_label[[i]] <- test$Outcome == 1
  
}
cross_pred <- prediction(pred_label, actual_label)
cross_perf <- performance(cross_pred,"tpr","fpr")
svmauc_c <- performance(cross_pred, "auc")
svmauc_c <- unlist(slot(svmauc_c, "y.values"))

plot(cross_perf,col="grey82",lty=3)
legend(0.6,0.9,c(c(paste('AUC is', svmauc_c)),"\n"),
       border="white",cex=0.5, box.col = "white")
plot(cross_perf, lwd=3,avg="vertical",spread.estimate="boxplot",add=T)
```


Row {data-height=300}
-----------------------------------------------------------------------
### Plot 3. RF Prediction Model Confusion Matrix
```{r}
# RF
rf_classifier = randomForest(Outcome ~ ., data=dftrain, ntree=100, mtry=2, importance=TRUE, probability = T)
prediction_for_table <- predict(rf_classifier,dftest[,-9], probability = T)
# table(observed=dftest$Outcome,predicted=prediction_for_table)
cmrf=confusionMatrix(prediction_for_table, dftest$Outcome, positive = "1")
draw_confusion_matrix(cmrf)
```


### Plot 4. RF Prediction Model 5-Folds Cross Validation ROC
```{r}
# Repeat using 10fold cross validation. Split data into 10 equally sized parts
folds <- function(x,n) split(x, sort(rep(1:n,len=length(x))))
crossdata <- folds(data1,5)
pred_label <- list()
actual_label <- list()
auc_list <- list()

for (i in 1:5) {
  test <- do.call("rbind", crossdata[i])
  train <- do.call("rbind", crossdata[-i])
  # print((train))
  rf_classifier <- randomForest(Outcome ~ ., data=dftrain, ntree=100, mtry=2, importance=TRUE, probability = T)
  pred_class_rf <- predict(rf_classifier,test[,-9], probability = T)


  pred_prob_rf <- predict(rf_classifier,test[,-9],type="prob")[,"1"]

  pred_label[[i]] <- pred_prob_rf
  actual_label[[i]] <- test$Outcome == 1
  
}
cross_pred <- prediction(pred_label, actual_label)
cross_perf <- performance(cross_pred,"tpr","fpr")
svmauc_c <- performance(cross_pred, "auc")
svmauc_c <- unlist(slot(svmauc_c, "y.values"))

plot(cross_perf,col="grey82",lty=3)
legend(0.6,0.9,c(c(paste('AUC is', svmauc_c)),"\n"),
       border="white",cex=0.5, box.col = "white")
plot(cross_perf, lwd=3,avg="vertical",spread.estimate="boxplot",add=T)
```




Test Your Chance of Having Diabetes
=======================================================================
Column {data-width=500}
-----------------------------------------------------------------------

### Information Collection
We need all 9 fields be filled out in order to make the best prediction.
Incomplete information might infer the model reliability. 



```{r}
sliderInput("Pregnancy", label = "Number of times pregnancy:",
            min=0,max=17,value=0,step=1)
numericInput("BMI", "BMI:", 20, min = 0, max = 81, step=0.1)
numericInput("Age", "Age:", 20, min = 0, max = 120, step=1)
numericInput("Glucose", "Plasma glucose concentration over 2 hours in an oral glucose tolerance test:", 10, min = 0, max = 200, step=1)
numericInput("BloodPressure", "Diastolic blood pressure (mm Hg):", 60, min = 0, max = 120, step=1)
numericInput("SkinThickness", "Triceps skin fold thickness (mm)", 10, min = 0, max = 110, step=1)
numericInput("Insulin", "2-Hour serum insulin (mu U/ml):", 10, min = 0, max = 744, step=1)
numericInput("PedigreePedigreeFunction", "Diabetes pedigree function (a function which scores likelihood of diabetes based on family history):", 0.5, min = 0, max = 1, step=0.001)


```

Column {data-width=500}
-----------------------------------------------------------------------

### Results from SVM Model

```{r}
test=reactive({
Pregnancies=input$Pregnancy
Glucose=input$Glucose
BloodPressure=input$BloodPressure
SkinThickness=input$SkinThickness
BMI=input$BMI
Insulin=input$Insulin
DiabetesPedigreeFunction=input$PedigreePedigreeFunction
Age=input$Age
test=data.frame(Pregnancies,Glucose,BloodPressure,SkinThickness,BMI,Insulin,DiabetesPedigreeFunction,Age)})
```

#### According to our calculation, your chance (out of 1) of having diabetes is:
```{r}
renderPrint({
prediction_svm <- predict(svm_model,test(), probability = T)
pred_prob_svm <- attr(prediction_svm, "probabilities")[,"1"]
print(pred_prob_svm)
})
renderText({
prediction_svm <- predict(svm_model,test(), probability = T)
pred_prob_svm <- attr(prediction_svm, "probabilities")
if (prediction_svm ==1){
  print('You are likely having Diabetes.')
}else{
  print('You do not seem to have Diabetes comparing to other patients in our database.')}
})
```

### Results from RF Model
#### According to our calculation, your chance (out of 1) of having diabetes is:
```{r}

renderPrint({

prediction_rf <- predict(rf_classifier,test(), probability = T)
prediction_prob_rf <- predict(rf_classifier,test(),type="prob")[,"1"]
print(prediction_prob_rf)

})
renderText({

prediction_rf <- predict(rf_classifier,test(), probability = T)
prediction_prob_rf <- predict(rf_classifier,test(),type="prob")
if (prediction_rf ==1){
  
  ('You are likely having Diabetes.')}
else{
  ('You do not seem to have Diabetes comparing to other patients in our database.')}
})
```



About the app
=======================================================================

- **Introduction:** Welcome to the diabetes prediction app! This app aims to provide predictions on the chance a user will develop diabetes which hopefully can remind users to be aware of the importance of having healthier diets and living habits to reduce the chance of developing diabetes. The two data sets are both from Kaggle and are data about the different factors that are associated with the development of diabetes. The first data set has 9 variables (8 factors & 1 outcome) and 2000 observations. It was used to train and test the first model and provide predictions on the chance of developing diabetes based on BMI, age, glucose level, blood pressure, and 4 other factors. The second data set has 17 variables (16 factors & 1 class/outcome) and 520 observations. It was used to train and test the second model and provide predictions on the chance of developing diabetes based on gender, obesity, muscle stiffness, sudden weight loss, delay healing, and 12 other factors. 


- **User guide:** There are a total of 4 panels in this app. The overview panel provides some basic statistics about the prevalence of diabetes in the United States in the past few years. The “complete model evaluation on 2000 patient data” panel gives the evaluations of the two models we built using the diabetes data set. The “” panel allows users to test their chance to develop diabetes based on the information they provide. The two prediction models used in this app are support-vector machine (SVM) and random forest (RF). After users inputting all the required information, both models will give predictions on the chance of developing diabetes. Last, this “about the app” panel provides introduction and user guide of this app to the users.


- **About the prediction model:** our first model was built using support-vector machine (SVM), and the second model was built using random forest (RF). Both models were built using the first data set and have accuracy above 80%.



- **Creators:** Shunyao Lei, Junyuan(Sandy) Shi, & Zihui Ou

- **Last updated:** 05/18/2021

- **References:** 
  + https://www.kaggle.com/sujan97/early-stage-diabetes-2020
  + https://www.kaggle.com/vikasukani/diabetes-data-set
  + https://www.cdc.gov/diabetes/pdfs/data/statistics/national-diabetes-statistics-report.pdf



