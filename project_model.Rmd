---
title: "DSBME_finalProject"
author: "Shunyao Lei"
date: "5/12/2021"
output: html_document
---

# Libraries
```{r global, include=FALSE}

# setwd("/Users/shunyaolei/RStudio/DSBME/project")
library(dplyr)
library(readr)
library(klaR) # NaiÌˆve Bayes
library(e1071) # SVM
library(ROCR) # ROC curves and AUC
library(VennDiagram)
library(RColorBrewer)
library(vioplot)
library(hfunk)
library(gplots) # heatmap.2
library(caret) # confusionmatrix
library(randomForest)

```

# Data
```{r warning=FALSE, message=FALSE}

# url1 = "https://raw.githubusercontent.com/ShunyaoLei/ds4ph_finalproject/main/diabetes-dataset.csv?token=AQ55A37EJI7DWULKGLXOLJ3ATPW2G"
# data1 = read_csv(url(url1))
# url2 = "https://raw.githubusercontent.com/ShunyaoLei/ds4ph_finalproject/main/diabetes_data_upload.csv?token=AQ55A363W6DRO6WRUZ7H27TATPW5C"
# data2 = read_csv(url(url2))
data1 = read_csv('diabetes-dataset.csv')
data2 = read_csv('diabetes_data_upload.csv')

```


```{r warning=FALSE, message=FALSE}
str(data1)
str(data2)
# data1<-data1[complete.cases(data1), ]
# data2<-data2[complete.cases(data2), ]
```

# data1
```{r}

# SVM
data1$Outcome = as.factor(data1$Outcome)
testidx <- which(1:(dim(data1[,1])[1])%%5 == 0)
dftrain <- data1[-testidx,]
dftest <- data1[testidx,]
hist(as.numeric(dftrain$Outcome))
hist(as.numeric(dftest$Outcome))
svm_model <- svm(Outcome~., data = dftrain, probability = T, kernel="polynomial", degree=5)
svm_prediction <- predict(svm_model, dftest, probability = T)
table(predicted=svm_prediction, observed=dftest$Outcome)
cm=confusionMatrix(svm_prediction, dftest$Outcome, positive = "1")
```
```{r}
pred_class_svm <- attr(svm_prediction, "probabilities")[, "1"]
# print(pred_class_svm)
# print(svm_prediction)
actual_class_svm <- dftest$Outcome == '1'
pred_svm <- prediction(pred_class_svm, actual_class_svm)
svmperf <- performance(pred_svm, "tpr", "fpr")
svmauc <- performance(pred_svm, "auc")
svmauc <- unlist(slot(svmauc, "y.values"))
plot(svmperf, colorize=TRUE)
title("SVM ROC curve and AUC")
legend(0.6,0.3,c(c(paste('AUC is', round(svmauc,3))),"\n"),
       border="white",cex=1.0, box.col = "white")
```
```{r}
draw_confusion_matrix <- function(cm) {

  layout(matrix(c(1,1,2)))
  par(mar=c(2,2,2,2))
  plot(c(100, 345), c(300, 450), type = "n", xlab="", ylab="", xaxt='n', yaxt='n')
  title('CONFUSION MATRIX', cex.main=2)

  # create the matrix 
  rect(150, 430, 240, 370, col='#3F97D0')
  text(195, 435, 'Class1', cex=1.2)
  rect(250, 430, 340, 370, col='#F7AD50')
  text(295, 435, 'Class2', cex=1.2)
  text(125, 370, 'Predicted', cex=1.3, srt=90, font=2)
  text(245, 450, 'Actual', cex=1.3, font=2)
  rect(150, 305, 240, 365, col='#F7AD50')
  rect(250, 305, 340, 365, col='#3F97D0')
  text(140, 400, 'Class1', cex=1.2, srt=90)
  text(140, 335, 'Class2', cex=1.2, srt=90)

  # add in the cm results 
  res <- as.numeric(cm$table)
  text(195, 400, res[1], cex=1.6, font=2, col='white')
  text(195, 335, res[2], cex=1.6, font=2, col='white')
  text(295, 400, res[3], cex=1.6, font=2, col='white')
  text(295, 335, res[4], cex=1.6, font=2, col='white')

  # add in the specifics 
  plot(c(100, 0), c(100, 0), type = "n", xlab="", ylab="", main = "DETAILS", xaxt='n', yaxt='n')
  text(10, 85, names(cm$byClass[1]), cex=1.2, font=2)
  text(10, 70, round(as.numeric(cm$byClass[1]), 3), cex=1.2)
  text(30, 85, names(cm$byClass[2]), cex=1.2, font=2)
  text(30, 70, round(as.numeric(cm$byClass[2]), 3), cex=1.2)
  text(50, 85, names(cm$byClass[5]), cex=1.2, font=2)
  text(50, 70, round(as.numeric(cm$byClass[5]), 3), cex=1.2)
  text(70, 85, names(cm$byClass[6]), cex=1.2, font=2)
  text(70, 70, round(as.numeric(cm$byClass[6]), 3), cex=1.2)
  text(90, 85, names(cm$byClass[7]), cex=1.2, font=2)
  text(90, 70, round(as.numeric(cm$byClass[7]), 3), cex=1.2)

  # add in the accuracy information 
  text(30, 35, names(cm$overall[1]), cex=1.5, font=2)
  text(30, 20, round(as.numeric(cm$overall[1]), 3), cex=1.4)
  text(70, 35, names(cm$overall[2]), cex=1.5, font=2)
  text(70, 20, round(as.numeric(cm$overall[2]), 3), cex=1.4)
}  

draw_confusion_matrix(cm)
```



```{r}
# RF
rf_classifier = randomForest(Outcome ~ ., data=dftrain, ntree=100, mtry=2, importance=TRUE, probability = T)
prediction_for_table <- predict(rf_classifier,dftest[,-9], probability = T)
table(observed=dftest$Outcome,predicted=prediction_for_table)
confusionMatrix(prediction_for_table, dftest$Outcome, positive = "1")

prediction_for_roc_curve <- predict(rf_classifier,dftest[,-9],type="prob")
# print(prediction_for_table)
# print(prediction_for_roc_curve)
pred_class_rf <- prediction_for_roc_curve[, "1"]
actual_class_rf <- dftest$Outcome == '1'
pred_rf <- prediction(pred_class_rf, actual_class_rf)
rfperf <- performance(pred_rf, "tpr", "fpr")
rfauc <- performance(pred_rf, "auc")
rfauc <- unlist(slot(rfauc, "y.values"))
plot(rfperf, colorize=TRUE)
title("RF ROC curve and AUC")
legend(0.6,0.3,c(c(paste('AUC is', round(rfauc,3))),"\n"),
       border="white",cex=1.0, box.col = "white")


```

# data2
```{r}

# SVM
data2$class = as.factor(data2$class)
names(data2)[c(5,8,9,12,13,14)]<-c("sudden_weight_loss","Genital_thrush","visual_blurring","delayed_healing","partial_paresis","muscle_stiffness")
testidx2 <- which(1:(dim(data2[,1])[1])%%5 == 0)
dftrain2 <- data2[-testidx2,]
dftest2 <- data2[testidx2,]
svm_model2 <- svm(class~., data = dftrain2, probability = T, kernel="polynomial", degree=5)
svm_prediction2 <- predict(svm_model2, dftest2, probability = T)
table(predicted=svm_prediction2, observed=dftest2$class)
confusionMatrix(svm_prediction2, dftest2$class, positive = "Positive")

pred_class_svm2 <- attr(svm_prediction2, "probabilities")[, "Positive"]
actual_class_svm2 <- dftest2$class == 'Positive'
pred_svm2 <- prediction(pred_class_svm2, actual_class_svm2)
svmperf2 <- performance(pred_svm2, "tpr", "fpr")
svmauc2 <- performance(pred_svm2, "auc")
svmauc2 <- unlist(slot(svmauc2, "y.values"))
plot(svmperf2, colorize=TRUE)
title("SVM ROC curve and AUC")
legend(0.6,0.3,c(c(paste('AUC is', round(svmauc2,3))),"\n"),
       border="white",cex=1.0, box.col = "white")

# RF

rf_classifier2 = randomForest(class ~ ., data=dftrain2, ntree=100, mtry=2, importance=TRUE, probability = T)
prediction_for_table2 <- predict(rf_classifier2,dftest2[,-17], probability = T)
table(observed=dftest2$class,predicted=prediction_for_table2)
confusionMatrix(prediction_for_table2, dftest2$class, positive = "Positive")

prediction_for_roc_curve2 <- predict(rf_classifier2,dftest2[,-17],type="prob")
pred_class_rf2 <- prediction_for_roc_curve2[, "Positive"]
actual_class_rf2 <- dftest2$class == 'Positive'
pred_rf2 <- prediction(pred_class_rf2, actual_class_rf2)
rfperf2 <- performance(pred_rf2, "tpr", "fpr")
rfauc2 <- performance(pred_rf2, "auc")
rfauc2 <- unlist(slot(rfauc2, "y.values"))
plot(rfperf2, colorize=TRUE)
title("RF ROC curve and AUC")
legend(0.6,0.3,c(c(paste('AUC is', round(rfauc2,3))),"\n"),
       border="white",cex=1.0, box.col = "white")
```


```{r}

# Repeat using 10fold cross validation. Split data into 10 equally sized parts
folds <- function(x,n) split(x, sort(rep(1:n,len=length(x))))
crossdata <- folds(data1,5)
pred_label <- list()
actual_label <- list()
auc_list <- list()

for (i in 1:5) {
  test <- do.call("rbind", crossdata[i])
  train <- do.call("rbind", crossdata[-i])
  # print((train))
  model <- svm(Outcome~., data = train, probability = T, kernel="linear")

  prediction <- predict(model, test, probability = T)
  pred_class_svm <- attr(prediction, "probabilities")[, "1"]
  table(test$Outcome, pred_class_svm)
  pred_label[[i]] <- pred_class_svm
  actual_label[[i]] <- test$Outcome == 1
  
}
cross_pred <- prediction(pred_label, actual_label)
cross_perf <- performance(cross_pred,"tpr","fpr")
svmauc_c <- performance(cross_pred, "auc")
svmauc_c <- unlist(slot(svmauc_c, "y.values"))

plot(cross_perf,col="grey82",lty=3)
legend(0.6,0.9,c(c(paste('AUC is', svmauc_c)),"\n"),
       border="white",cex=0.5, box.col = "white")
plot(cross_perf, lwd=3,avg="vertical",spread.estimate="boxplot",add=T)

```

```{r}
# Repeat using 10fold cross validation. Split data into 10 equally sized parts
folds <- function(x,n) split(x, sort(rep(1:n,len=length(x))))
crossdata <- folds(data1,5)
pred_label <- list()
actual_label <- list()
auc_list <- list()

for (i in 1:5) {
  test <- do.call("rbind", crossdata[i])
  train <- do.call("rbind", crossdata[-i])
  # print((train))
  rf_classifier <- randomForest(Outcome ~ ., data=dftrain, ntree=100, mtry=2, importance=TRUE, probability = T)
  pred_class_rf <- predict(rf_classifier,test[,-9], probability = T)
  # table(observed=test$Outcome,predicted=prediction_for_table)
  # confusionMatrix(prediction_for_table, dftest$Outcome, positive = "1")

  pred_prob_rf <- predict(rf_classifier,test[,-9],type="prob")[,"1"]
# print(prediction_for_table)
# print(prediction_for_roc_curve)
  # pred_class_rf <- prediction_for_roc_curve[, "1"]
  # prediction <- predict(model, test, probability = T)
  # pred_class_svm <- attr(prediction, "probabilities")[, "1"]
  # table(test$Outcome, pred_prob_rf)
  pred_label[[i]] <- pred_prob_rf
  actual_label[[i]] <- test$Outcome == 1
  
}
cross_pred <- prediction(pred_label, actual_label)
cross_perf <- performance(cross_pred,"tpr","fpr")
svmauc_c <- performance(cross_pred, "auc")
svmauc_c <- unlist(slot(svmauc_c, "y.values"))

plot(cross_perf,col="grey82",lty=3)
legend(0.6,0.9,c(c(paste('AUC is', svmauc_c)),"\n"),
       border="white",cex=0.5, box.col = "white")
plot(cross_perf, lwd=3,avg="vertical",spread.estimate="boxplot",add=T)
```

